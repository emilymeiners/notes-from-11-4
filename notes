---
title: "PSY4960- Statistics in R"
author: "Amanda Mae Woodward"
date: "11/2/2021"
output: html_document
---
# Learning Outcomes:
By the end of today's class, students should be able to: 
- obtain descriptive statistics in R
- conduct common parametric analyses in R
- conduct common nonparametric analyses in R

**Disclaimer:** Covering every type of analysis in R could be an entire course by itself. Today, we'll cover **some** analyes you can do. If there are additional analyses you'd like to cover, please let me know and I'm happy to upload supplemental code or cover it in a later class (there is flexibility in the last couple of weeks!). 

Additionally, we will **not** cover interpretations in depth in this class. The goal is to teach you how to use R to run the tests, and adding interpretations for each test could make this into several semester long courses. However, if you have questions about how to interpret statistics, please let me know and I can adjust our course material. I am happy to talk about interpretations in office hours, or you will learn about them in your statistics courses.

We'll use the following data throughout today's class:
```{r}
dat<- read.csv("~/Desktop/PSY4960-StatisticsData.csv")
```
```{r}
set.seed(100)
subj<- 1:60
conditionFirst<- c(rep("Exclusion", 30), rep("Inclusion", 30))
age<- rep(c(rep(4, 10), rep(5, 10),rep(6, 10)),2)
type<- rep(c("observed", "experienced"), 30)
evaluationExclusion<- sample(1:5, 60, replace=TRUE)
evaluationInclusion<- sample(1:5, 60, replace=TRUE)
detectionExclusion<- sample(c("incorrect", "correct"), 60, replace=TRUE)
detectionInclusion<- sample(c("incorrect", "correct"), 60, replace=TRUE)
playChoice<- sample(c("includer", "excluder"), 60,replace=TRUE)
dat<-cbind.data.frame(subj, conditionFirst,age, type, evaluationExclusion, evaluationInclusion, detectionExclusion, detectionInclusion, playChoice)
```


### Learning Outcome 1: Obtaining descriptive statistics in R
We've gone through some of these already, but I want to make sure we're on the same page. For descriptive statistics, we'll mostly focus on the measures of central tendency and measures of variability. 

#### Central Tendency
**mean** 
```{r}
mean(dat$evaluationExclusion)
```
**median**
```{r}
median(dat$evaluationExclusion)
```
**mode**
```{r}
library(modeest)

mfv(dat$evaluationExclusion)
```

#### Variability
**range**
```{r}
range(dat$evaluationExclusion)
```
**interquartile range**
```{r}
IQR(dat$evaluationExclusion)
```
**standard deviation**
```{r}
sd(dat$evaluationExclusion)
```
**variance**
```{r}
var(dat$evaluationExclusion)
```
***summary***
```{r}
summary(dat$evaluationExclusion)
```
#### z score
The other thing that we'll put in this section is how to create a z score in your data. This allows us to view one score relative to others, even if they are collected from different distributions

```{r}
scale(dat$evaluationExclusion)
```

##### Learning Outcome 1 Practice
1) calculate the mean, median, and mode for the inclusion evaluations
```{r}
mean(dat$evaluationInclusion)
median(dat$evaluationInclusion)
mfv(dat$evaluationInclusion)

```
2) what do you notice about these scores? (are they the same? different?)
```{r}
#different from each other


library(ggplot2)
ggplot(dat, aes(evaluationInclusion))+geom_histogram()
```
3) create z scores for the inclusion evaluations. Interpret what participant 3's z score means. 
```{r}
scale(dat$evaluationInclusion)

```

### Learning Outcome 2: Conduct common parametric analyses in R
Now that we have covered some descriptive statistics, we'll talk about parametric ones. Parametric statistics are those that rely on assumptions to make inferences from the sample to the population. We'll go through correlations, t-tests, regression, and ANOVA. We'll go through nonparametric tests, or those that rely on less assumptions, in the next section. 

#### Pearson correlation
We'll practice running correlations using the dataset above. To do this, we'll look at the correlation between Evaluations for includers and excluders in the dataset. 
cor(x, y)
```{r}
cor(dat$evaluationExclusion,dat$evaluationInclusion)
```
**Note:** It's great that we can see the correlation between these two measures, but we don't have any additional information, ie information related to significance.We can use another function, cor.test, to get information about significance.
cor.test(x,y)
```{r}
cor.test(dat$evaluationInclusion, dat$evaluationInclusion)
```
We can change whether we our conducting a one tailed or a two tailed test by including an additional argument "alternative." It defaults to a two tailed test, but we can specify a one tailed test in either direction (greater or less) 
```{r}
cor.test(dat$evaluationInclusion, dat$evaluationInclusion, alternative="greater")
cor.test(dat$evaluationInclusion, dat$evaluationInclusion, alternative="less")
```

#### t-tests
We can run a variety of t-tests using the same function t.test(). 
##### one sample t-test
A one sample t test can be computed by specifying mu in the arguments. 
t.test(variable, mu)
```{r}
t.test(dat$evaluationExclusion, mu=2.5)
```

##### two samples t-test
There are two ways we can use this function when we have two variables (independent or paired). The first is to type our x and y variables in as we did in the correlation function above. 
```{r}
t.test(dat$evaluationExclusion,dat$evaluationInclusion)
```
OR
we can type them in as a formula in R. Formulas typically take the form y ~ x. To show you this example, I need to reformat our wide data to long data (using what we did last week!)

```{r}
library(tidyr)
datLong<- pivot_longer(dat, cols=c("evaluationExclusion","evaluationInclusion"), names_to="personEvaluated",values_to="evaluation")
```
This changes the t test arguments to: 
t.test(formula, data)
y~x
```{r}
t.test(evaluation~ personEvaluated,data=datLong)

#only for long data
```
If we think about the structure of our data, our data seem to be from a repeated measure. It's okay if you didn't infer that, it's my data and I made them to be this way. This is why including readme files for your data (whether it's publicly available or you're sharing it with collaborators) is really important! 

To account for the lack of dependence between observations, we'll run a paired samples t test. The code looks pretty similar to above, but we'll use an additional argument. 
```{r}
t.test(evaluation~ personEvaluated,data=datLong,paired=TRUE)
```

You'll notice that the top of the t-test output says "Welch's Two sample t-test." This R function automatically assumes that the variances of each group are unequal. If we wanted to run a traditional paired-samples t-test, we need to include another argument. 
```{r}
t.test(evaluation~personEvaluated, data=datLong, var.equal=TRUE)
```

Finally, we some times run one tailed vs two tailed tests, just like we did with the correlations. 
```{r}
t.test(evaluation~personEvaluated,data=datLong,var.equal=TRUE,alternative="greater")
```
Differences between Greater and Less:
```{r}

```

##### Correlation and T-test practice
1. Open the mtcars dataset. Find the correlation between mpg and hp
```{r}
data(mtcars)

cor(mtcars$mpg,mtcars$hp)

```

2. Conduct a significance test to determine if displacement and miles per gallon are significantly correlated. 
```{r}
cor.test(mtcars$disp,mtcars$mpg)
#there is a significant relationship, the p-value is <0.05
```

3. Conduct a two-tailed t-test examining whether the average mpg differs by transmission (am). 
```{r}
t.test(mpg~am,data=mtcars,var.equal=TRUE)
#it is significant,the p-value is <0.05
```

4. Conduct a one-tailed t-test examining whether the average displacement(disp) differs  engine shape (vs). Specifically, test whether straight engines result in higher displacements.
```{r}
t.test(disp~vs,data=mtcars,var.equal=TRUE,alternative="greater")

or

t.test(mtcars$disp[mtcars$vs==1],mtcars$disp[mtcars$vs==1],var.equal=TRUE,alternative="greater")
```

###Extra Code about Correlation Tables 
cor() can also be used to create correlation matrices, but need to create a dataframe that is just the variables you'd like to use. 
cor(dat)
we'll do a quick example w/ mt cars
```{r}
cor(mtcars)

View(cor(mtcars))
```

#### regression
Back to the Exclusion Dataset we imported. The code for a linear regression is really similar (ie identical)  to what we used for t-tests.
lm(DV ~ IV, data)
```{r}

lm(evaluationExclusion~conditionFirst,data=dat)

#intercept is the y intercept, conditionFirstInclusion=slope, for this one the equation yould be y=0.1333+3.0667
#x=0 is exlusion first, x=1 is inclusion first
#exclusion comes before inclusion, so if you're looking to get the other one first, you have to relevel or just get rid of the slope
dat$conditionFirst<- as.factor(dat$conditionFirst)
dat
dat$conditionFirst<- relevel(dat$conditionFirst, ref="Inclusion")
lm(evaluationExclusion~conditionFirst, data=dat)
```
I tend to save my linear models because it allows me to do a few useful things:
Just like we used summary to get a summary of our data, we can use the same function to learn more about our models
```{r}

lm1<-lm(evaluationExclusion~conditionFirst,data=dat)
summary(lm1)
```
str() is a new(ish) function that allows us to learn about the structure of our model. We can use this to get specific pieces of information, or additional information that "underlies" our model (eg residuals and fitted values)
```{r}
str(lm1)

#residuals is the error, its at the top(ish) of the list of the code that was just ran, there are other thigs in this list you can get too, just treat it as a column in it (the names are at the top of the code just ran)

lm1$residuals
lm1$fitted.values
lm1$coefficients
```

**Multiple Regression**
We can include additional factors and interaction terms to our models: 

```{r}
#y=bx(1)+bx(2)

multReg<- lm(evaluationExclusion~age + evaluationInclusion, data=dat)

#the plus sign adds in a second variable

summary(multReg)
```
The : can be used instead of + to include an interaction in your model
```{r}
multReg2<- lm(evaluationExclusion~age+evaluationInclusion+ age:evaluationInclusion,data=dat)

summary(multReg2)
```

Using * instead of + will include both the individual predictors AND their interactions 
```{r}
multReg2<- lm(evaluationExclusion~age*evaluationInclusion,data=dat)
summary(multReg2)
```

The class of our data and the way data are entered matter for regression models. 
let's consider age:
```{r}

```
Data don't really look continuous. We can change age to a factor. This will influence our output.  
```{r}

```

We may also need to change the reference level for factors. For instance, in this case, we care about how age relates to children's evaluations of excluders. 
relevel(dat$age, ref="x")
```{r}

```

**Anova**
There are several ways you can get Anova results in R. There are differences in the ways that they handle interactions, but they are used in the same way. 
```{r}
"aov()"
"anova()"
library(car)

linearM<- lm(evaluationExclusion~age,data=dat)
Anova(linearM)
```
